{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sketching Methods Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm1ZjOrDi6SY",
        "colab_type": "text"
      },
      "source": [
        "# Sketching Methods for Analysis of Matrices and Data (Fall 2019) - Final Project\n",
        "## Comparison of Least-Square Solvers - Elad Eatah\n",
        "\n",
        "### TODO: Update link to the notebook in the repo!\n",
        "<a href=\"????\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n",
        "[![MIT License](https://img.shields.io/apm/l/atomic-design-ui.svg?)](https://github.com/tterb/atomic-design-ui/blob/master/LICENSEs)\n",
        "\n",
        "Complete description of this project is avilable in [this repo](https://github.com/RedCrow9564/SketchingMethodsInDataAnalysis-Final-Project/tree/develop).\n",
        "\n",
        "## Getting Started\n",
        "First run the nodes under \"Infrastructure\" and \"Compared Algorithms\" one by one.\n",
        "Second, run the first two nodes under \"Main Components\".\n",
        "\n",
        "### Running Unit-Tests\n",
        "You can then run the nodes under \"Unit-Tests\" one by one. The results of the last node are the results of these Unit-Tests.\n",
        "\n",
        "### Performing an experiment\n",
        "This is done by running the \"Main\" node. The configuration of the experiment\n",
        "can be set in the \"config\" function in the \"Main\" node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKcTzId7nVSa",
        "colab_type": "text"
      },
      "source": [
        "##Infrastructure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYayQGyEk2up",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "3793dab1-b1a4-4dbc-881a-2f1fc3c92f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@title Dependencies installations\n",
        "!pip install intel-numpy \n",
        "!pip install intel-scipy \n",
        "!pip install intel-scikit-learn\n",
        "!pip install nptyping\n",
        "!pip install sacred\n",
        "!pip install Cython\n",
        "from IPython.display import clear_output\n",
        "from nptyping import Array\n",
        "from typing import List, Dict, Callable, ClassVar, Union, Iterator\n",
        "import numpy as np\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import os\n",
        "import inspect\n",
        "from multiprocessing import Pool\n",
        "from time import time, perf_counter\n",
        "from sacred import Experiment\n",
        "#warnings.filterwarnings(\"error\")  # Uncomment to see warnings as errors.\n",
        "%load_ext Cython\n",
        "cpu_count: int = psutil.cpu_count(logical=True)\n",
        "clear_output()\n",
        "print(f'Total CPUs: {cpu_count}')\n",
        "print(\"Installation is done!\")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total CPUs: 4\n",
            "Installation is done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ2wKEzgi0AX",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "ba57f296-bc83-4152-ca43-6ff4e8d6f9c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Utils\n",
        "ex = Experiment(name=\"Initializing project\", interactive=True)\n",
        "\n",
        "\n",
        "# Naming data types for type hinting.\n",
        "Number = Union[int, float]\n",
        "Scalar = Union[Number, Array[float, 1, 1], Array[int, 1, 1]]\n",
        "RowVector = Union[List[Scalar], Array[float, 1, ...], Array[int, 1, ...], Scalar]\n",
        "ColumnVector = Union[List[Scalar], Array[float, ..., 1], Array[int, ..., 1], Scalar]\n",
        "Vector = Union[RowVector, ColumnVector]\n",
        "Matrix = Union[List[Vector], Array[float], Array[int], Vector, Scalar]\n",
        "StaticField = ClassVar\n",
        "\n",
        "\n",
        "class _MetaEnum(type):\n",
        "    \"\"\"\n",
        "    A private meta-class which given any :class:`BaseEnum` object to be an iterable.\n",
        "    This can be used for iterating all possible values of this enum. Should not be used explicitly.\n",
        "    \"\"\"\n",
        "    def __iter__(self) -> Iterator:\n",
        "        \"\"\"\n",
        "        This method gives any BaseEnum the ability of iterating over all the enum's values.\n",
        "\n",
        "        Returns:\n",
        "            An iterator for the collection of all the enum's values.\n",
        "\n",
        "        \"\"\"\n",
        "        # noinspection PyUnresolvedReferences\n",
        "        return self.enum_iter()\n",
        "\n",
        "    def __contains__(self, item) -> bool:\n",
        "        \"\"\"\n",
        "        This method give any BaseEnum the ability to test if a given item is a possible value for this enum class.\n",
        "\n",
        "        Returns:\n",
        "            A flag which indicates if 'item' is a possible value for this enum class.\n",
        "\n",
        "        \"\"\"\n",
        "        # noinspection PyUnresolvedReferences\n",
        "        return self.enum_contains(item)\n",
        "\n",
        "\n",
        "class BaseEnum(metaclass=_MetaEnum):\n",
        "    \"\"\"\n",
        "    A basic interface for all enum classes. Should be sub-classed in eny enum, i.e ``class ExperimentType(BaseEnum)``\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def enum_iter(cls) -> Iterator:\n",
        "        \"\"\"\n",
        "        This method gives any BaseEnum the ability of iterating over all the enum's values.\n",
        "\n",
        "        Returns:\n",
        "            An iterator for the collection of all the enum's values.\n",
        "\n",
        "        \"\"\"\n",
        "        return iter(cls.get_all_values())\n",
        "\n",
        "    @classmethod\n",
        "    def enum_contains(cls, item) -> bool:\n",
        "        \"\"\"\n",
        "        This method give any BaseEnum the ability to test if a given item is a possible value for this enum class.\n",
        "\n",
        "        Returns:\n",
        "            A flag which indicates if 'item' is a possible value for this enum class.\n",
        "\n",
        "        \"\"\"\n",
        "        return item in cls.get_all_values()\n",
        "\n",
        "    @classmethod\n",
        "    def get_all_values(cls) -> List:\n",
        "        \"\"\"\n",
        "        A method which fetches all possible values of an enum. Used for iterating over an enum.\n",
        "\n",
        "        Returns:\n",
        "            A list of all possible enum's values.\n",
        "\n",
        "        \"\"\"\n",
        "        all_attributes: List = inspect.getmembers(cls, lambda a: not inspect.ismethod(a))\n",
        "        all_attributes = [value for name, value in all_attributes if not (name.startswith('__') or name.endswith('__'))]\n",
        "        return all_attributes\n",
        "\n",
        "\n",
        "def create_factory(possibilities_dict: Dict[str, Callable], are_methods: bool = False) -> Callable:\n",
        "    \"\"\"\n",
        "    A generic method for creating factories for the entire project.\n",
        "\n",
        "    Args:\n",
        "        possibilities_dict(Dict[str, Callable]): The dictionary which maps object types (as strings!) and returns the\n",
        "            relevant class constructors.\n",
        "        are_methods(bool): A flag, true if the factory output are methods, rather than objects. Defaults to False\n",
        "\n",
        "    Returns:\n",
        "         The factory function for the given classes/methods mapping.\n",
        "\n",
        "    \"\"\"\n",
        "    def factory_func(requested_object_type: str):  # Inner function!\n",
        "        if requested_object_type not in possibilities_dict:\n",
        "            raise ValueError(\"Object type {0} is NOT supported\".format(requested_object_type))\n",
        "        else:\n",
        "            if are_methods:\n",
        "                return possibilities_dict[requested_object_type]\n",
        "            else:\n",
        "                return possibilities_dict[requested_object_type]()\n",
        "\n",
        "    return factory_func\n",
        "\n",
        "\n",
        "def compute_parallel(func: Callable, arguments) -> List:\n",
        "    \"\"\"\n",
        "    A method which computes a given method for given arguments in parallel and returns its result in a list.\n",
        "\n",
        "    Args:\n",
        "        func(Callable): A function whose computation will be performed in parallel.\n",
        "        arguments:  A collection for given inputs to the input method 'func'.\n",
        "\n",
        "    Returns:\n",
        "        A list for all outputs of this method for all given input arguments.\n",
        "\n",
        "    \"\"\"\n",
        "    pool = Pool()\n",
        "    results: List = pool.starmap(func, arguments)\n",
        "    pool.close()\n",
        "    return results\n",
        "\n",
        "\n",
        "def compute_serial(func: Callable, arguments) -> List:\n",
        "    \"\"\"\n",
        "        A method which computes a given method for given arguments in parallel and returns its result in a list.\n",
        "\n",
        "        Args:\n",
        "            func(Callable): A function whose computation will be performed in parallel.\n",
        "            arguments:  A collection for given inputs to the input method 'func'.\n",
        "\n",
        "        Returns:\n",
        "            A list for all outputs of this method for all given input arguments.\n",
        "\n",
        "        \"\"\"\n",
        "    results = list()\n",
        "    for argument in arguments:\n",
        "        results.append(func(*argument))\n",
        "    return results\n",
        "\n",
        "\n",
        "def measure_time(method: Callable) -> Callable:\n",
        "    \"\"\"\n",
        "    A method which receives a method and returns the same method, while including run-time measure\n",
        "    output for the given method, in seconds.\n",
        "\n",
        "    Args:\n",
        "        method(Callable): A method whose run-time we are interested in measuring.\n",
        "\n",
        "    Returns:\n",
        "        A function which does exactly the same, with an additional run-time output value in seconds.\n",
        "\n",
        "    \"\"\"\n",
        "    def timed(*args, **kw):\n",
        "        ts = perf_counter()\n",
        "        result = method(*args, **kw)\n",
        "        te = perf_counter()\n",
        "        duration_in_seconds: float = te - ts\n",
        "        return (result, duration_in_seconds)\n",
        "    timed.__name__ = method.__name__ + \" with time measure\"\n",
        "    return timed\n",
        "\n",
        "\n",
        "def is_empty(collection: List) -> bool:\n",
        "    return len(collection) == 0\n",
        "\n",
        "\n",
        "class DataLog:\n",
        "    \"\"\"\n",
        "    A class for log-management objects. See the following example for creating it: ``DataLog([\"Column 1\", \"Column 2\"])``\n",
        "    \"\"\"\n",
        "    def __init__(self, log_fields: List):\n",
        "        \"\"\"\n",
        "        This methods initializes an empty log.\n",
        "\n",
        "        Args:\n",
        "            log_fields(List) - A list of column names for this log.\n",
        "\n",
        "        \"\"\"\n",
        "        self._data: Dict = dict()\n",
        "        self._log_fields: List = log_fields\n",
        "        for log_field in log_fields:\n",
        "            self._data[log_field] = list()\n",
        "\n",
        "    def append(self, data_type: str, value: Vector) -> None:\n",
        "        \"\"\"\n",
        "        This methods appends given data to the given column inside the log.\n",
        "        Example of usage:``log.append(DataFields.DataSize, 20)``\n",
        "\n",
        "        Args:\n",
        "            data_type(LogFields): The column name in which the input data in inserted to.\n",
        "            value(Scalar): The value to insert to the log.\n",
        "\n",
        "        \"\"\"\n",
        "        self._data[data_type] += value\n",
        "\n",
        "    def append_dict(self, data_dict: Dict) -> None:\n",
        "        for log_field, data_value in data_dict.items():\n",
        "            self.append(log_field, data_value)\n",
        "\n",
        "    def save_log(self, log_file_name: str, results_folder_path: str) -> None:\n",
        "        \"\"\"\n",
        "        This method saves the log to a file, with the input name, in the input folder path.\n",
        "\n",
        "        Args:\n",
        "            log_file_name(str): The name for this log file.\n",
        "            results_folder_path(str): The path in which this log will be saved.\n",
        "\n",
        "        \"\"\"\n",
        "        df = pd.DataFrame(self._data, columns=self._log_fields)\n",
        "        df.to_csv(os.path.join(results_folder_path, log_file_name + \".csv\"), sep=\",\", float_format=\"%.2E\", index=False)\n",
        "        ex.info[\"Experiment Log\"] = self._data\n",
        "\n",
        "print(\"Utils loaded successfully!\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Utils loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8xtCs1FkHSX",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "3691f26d-ece3-433f-b8d4-07e8d982e0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Enums\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "enums.py - All enums section\n",
        "============================\n",
        "\n",
        "This module contains all possible enums of this project. Most of them are used by the configuration section in\n",
        ":mod:`main`. An example for using enum: ``ExperimentType.RunTimeExperiment``\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class LogFields(BaseEnum):\n",
        "    \"\"\"\n",
        "    The enum class of fields within experiments logs.\n",
        "    \"\"\"\n",
        "    DataSize: str = \"Data size\"\n",
        "    Coefficients: str = \"Coefficients\"\n",
        "    Residuals: str = \"Residuals\" #  2-norm of errors of estimation for the given data.\n",
        "    DurationInSeconds: str = \"Duration in seconds\"\n",
        "    AtTimesErrors: str = \"A transpose times Errors\"\n",
        "    AlphasCount: str = \"Alphas count\"\n",
        "\n",
        "\n",
        "class DatabaseType(BaseEnum):\n",
        "    \"\"\"\n",
        "    The enum class of dataset type to use in an experiment.\n",
        "    \"\"\"\n",
        "    Synthetic: str = \"Synthetic\"  # A random data\n",
        "\n",
        "    # 3D road network with highly accurate elevation information (+-20cm) from Denmark,\n",
        "    # used in eco-routing and fuel/Co2-estimation routing algorithms.\n",
        "    # See https://archive.ics.uci.edu/ml/datasets/3D+Road+Network+(North+Jutland,+Denmark)\n",
        "    ThreeDRoadNetwork: str = \"3D Road Network\"\n",
        "\n",
        "    # It includes homes sold between May 2014 and May 2015. See https://www.kaggle.com/harlfoxem/housesalesprediction\n",
        "    HouseSalesInKingCounty: str = \"House Sales in King County, USA\"\n",
        "\n",
        "    # Measurements of electric power consumption in one household with a one-minute sampling rate over a period of\n",
        "    # almost 4 years. This archive contains 2075259 measurements gathered in a house located in Sceaux\n",
        "    # #(7km of Paris, France) between December 2006 and November 2010 (47 months).\n",
        "    # See https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption.\n",
        "    IndividualHouseholdElectricPowerConsumption: str = \"Individual household electric power consumption\"\n",
        "\n",
        "\n",
        "class ExperimentType(BaseEnum):\n",
        "    \"\"\"\n",
        "    The enum class of experiment types.\n",
        "    \"\"\"\n",
        "    RunTimeExperiment: str = \"Run-Time Experiment\"\n",
        "    AccuracyExperiment: str = \"Accuracy Experiment\"\n",
        "    NumberOfAlphasExperiment: str = \"Number of Alphas Experiment\"\n",
        "\n",
        "\n",
        "class AlgorithmsType(BaseEnum):\n",
        "    \"\"\"\n",
        "    The enum class of algorithms type to be compared in an experiment.\n",
        "    \"\"\"\n",
        "    LinearRegression: str = \"Linear Regression\"\n",
        "    LassoRegression: str = \"Lasso Regression\"\n",
        "    RidgeRegression: str = \"Ridge Regression\"\n",
        "    ElasticNetRegression: str = \"ElasticNet Regression\"\n",
        "\n",
        "\n",
        "class LinearRegressionMethods(BaseEnum):\n",
        "    \"\"\"\n",
        "    The enum class of possible Linear-Regression solvers for experiments.\n",
        "    \"\"\"\n",
        "    BoostedSVDSolver: str = \"SVD solver with Caratheodory Coreset Booster\"\n",
        "    SVDBased: str = \"Based on SVD\"  # Numpy's standard linear-regression solver- lstsq, based on SVD decomposition.\n",
        "    QRBased: str = \"Based on QR\"  # Solver based on QR decomposition.\n",
        "    NormalEquationsBased: str = \"Based on Normal-Equations\"  # Solver based on solving the Normal-Equations.\n",
        "    SketchAndCholesky: str = \"Sketch + Cholesky\"\n",
        "    SketchAndInverse: str = \"Sketch + Inverse\"\n",
        "    SketchPreconditioned: str = \"Sketch Preconsitioning for LSQR\"\n",
        "\n",
        "\n",
        "class LassoRegressionMethods(BaseEnum):\n",
        "    \"\"\"\n",
        "    The enum class of possible Lasso-Regression solvers for experiments.\n",
        "    \"\"\"\n",
        "    SkLearnLassoRegression: str = \"Scikit-Learn's LassoCV Method\"  # Scikit-learn standard Lasso-Regression solver.\n",
        "\n",
        "    # The solver of Scikit-Learn, boosted by Caratheodory coreset booster.\n",
        "    BoostedLassoRegression: str = \"LassoCV with Fast Caratheodory booster\"\n",
        "\n",
        "\n",
        "class RidgeRegressionMethods(BaseEnum):\n",
        "    \"\"\"\n",
        "    The enum class of possible Ridge-Regression solvers for experiments.\n",
        "    \"\"\"\n",
        "    SkLearnRidgeRegression: str = \"Scikit-Learn's RidgeCV Method\"  # Scikit-learn standard Ridge-Regression solver.\n",
        "\n",
        "    # The solver of Scikit-Learn, boosted by Caratheodory coreset booster.\n",
        "    BoostedRidgeRegression: str = \"RidgeCV with Fast Caratheodory booster\"\n",
        "\n",
        "\n",
        "class ElasticNetRegressionMethods(BaseEnum):\n",
        "    \"\"\"\n",
        "    The enum class of possible Elastic-Net-Regression solvers for experiments.\n",
        "    \"\"\"\n",
        "    # Scikit-learn standard Elastic-Net-Regression solver.\n",
        "    SkLearnElasticNetRegression: str = \"Scikit-Learn's ElasticNetCV Method\"\n",
        "\n",
        "    # The solver of Scikit-Learn, boosted by Caratheodory coreset booster.\n",
        "    BoostedElasticNetRegression: str = \"ElasticNetCV with Fast Caratheodory booster\"\n",
        "\n",
        "\n",
        "class NumpyDistribution(BaseEnum):\n",
        "    \"\"\"\n",
        "    The type of Numpy distribution used within an experiment. Used for experiment documentation only.\n",
        "    \"\"\"\n",
        "    IntelDistribution: str = \"Intel's Distribution\"\n",
        "    CPythonDistribution: str = \"CPython's distribution\"\n",
        "\n",
        "\n",
        "print(\"Enums loaded successfully!\")\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enums loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_D2B6G-V3YY",
        "colab_type": "text"
      },
      "source": [
        "#Compared Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NzJ1KnsrCUR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title base-least-square-solver\n",
        "\n",
        "class BaseSolver(object):\n",
        "    def __init__(self, data_features: Matrix, output_samples: ColumnVector, n_alphas: int,\n",
        "                 cross_validation_folds: int):\n",
        "        self._data_features = data_features\n",
        "        self._output_samples = output_samples\n",
        "        self._cross_validation_folds = cross_validation_folds\n",
        "        self._n_alphas = n_alphas\n",
        "        self._model = None\n",
        "        self._fitted_coefficients: ColumnVector = None\n",
        "\n",
        "    def fit(self):\n",
        "        raise NotImplementedError(\"Any subclass MUST implement this method!\")\n",
        "\n",
        "    def calc_residuals(self):\n",
        "        return self._output_samples - self._data_features.dot(self._fitted_coefficients)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB6H2c-4aoCU",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "c98ed4e4-a4bb-428b-856c-7c17bcf5ef25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Generate Preconditioner Sketch Method\n",
        "%%cython -f\n",
        "# distutils: extra_compile_args = -fopenmp\n",
        "# distutils: extra_link_args = -fopenmp\n",
        "# cython: language_level = 3, boundscheck = False, wraparound = False\n",
        "# cython: initializedcheck = False, nonecheck = False\n",
        "\n",
        "cimport openmp\n",
        "import numpy as np\n",
        "cimport numpy as np\n",
        "from cython.parallel cimport prange\n",
        "from numpy.math cimport INFINITY\n",
        "from libc.stdlib cimport rand, RAND_MAX, srand, malloc, free, abort\n",
        "from scipy.fft import dct\n",
        "\n",
        "cdef inline void _random_sign_change(const double[:, ::1] data_matrix, double[:, ::1] sketched_mat,\n",
        "                                     const double switch_sign_probability) nogil:\n",
        "    cdef Py_ssize_t rows_num = data_matrix.shape[0]\n",
        "    cdef Py_ssize_t column_num = data_matrix.shape[1]\n",
        "    cdef Py_ssize_t i, j\n",
        "\n",
        "    for i in range(rows_num):\n",
        "        if <double>rand() / <double>RAND_MAX <= switch_sign_probability:\n",
        "            for j in range(column_num):\n",
        "                sketched_mat[i, j] = -data_matrix[i, j]\n",
        "        else:\n",
        "            for j in range(column_num):\n",
        "                sketched_mat[i, j] = data_matrix[i, j]\n",
        "\n",
        "cdef inline double[:, ::1] _pick_random_rows(double[:, ::1] sketched_mat, const unsigned int sampled_rows):\n",
        "    cdef Py_ssize_t* picked_rows = <Py_ssize_t*> malloc(sizeof(Py_ssize_t) * sampled_rows)\n",
        "    cdef Py_ssize_t[:] picked_rows_view\n",
        "    cdef Py_ssize_t total_rows_num = sketched_mat.shape[0]\n",
        "    cdef int row_index\n",
        "    if picked_rows == NULL:\n",
        "        abort()\n",
        "\n",
        "    for row_index in prange(sampled_rows, nogil=True, schedule=\"static\"):\n",
        "        picked_rows[row_index] = <Py_ssize_t>(rand() % total_rows_num)\n",
        "\n",
        "    picked_rows_view = <Py_ssize_t[:sampled_rows]> picked_rows\n",
        "    sketched_mat = sketched_mat.base[picked_rows_view, :]  # Unfortunately, this is a Python object call...\n",
        "    free(picked_rows)\n",
        "    return sketched_mat\n",
        "\n",
        "\n",
        "def generate_sketch_preconditioner(const double[:, ::1] data_matrix, const unsigned int sampled_rows,\n",
        "                                   double[:, ::1] sketched_mat, const unsigned int seed,\n",
        "                                   const double switch_sign_probability):\n",
        "    srand(seed)  # Seeding the random number generator.\n",
        "    _random_sign_change(data_matrix, sketched_mat, switch_sign_probability)\n",
        "    sketched_mat = dct(sketched_mat, norm='ortho')\n",
        "    #return _pick_random_rows(sketched_mat, sampled_rows)  # TODO: Check run time in Google Colab.\n",
        "    return sketched_mat.base[np.random.randint(low=0, high=sketched_mat.shape[0], size=sampled_rows), :]\n",
        "\n",
        "print(\"Loaded Sketch-Preconditioner Method successfully!\")\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Sketch-Preconditioner Method successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j-VqoQds95V",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "d9c89741-b210-445b-ad36-38ab128cf654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Basic Caratheodory-Booster in Cython\n",
        "%%cython -f\n",
        "# cython: language_level=3, boundscheck=False, wraparound=False\n",
        "# cython: initializedcheck=False, cdivision=True, nonecheck=False\n",
        "cimport openmp\n",
        "import numpy as np\n",
        "cimport numpy as np\n",
        "from cython.parallel import prange\n",
        "from numpy.math cimport INFINITY\n",
        "\n",
        "\n",
        "def cluster_average(const double[::1, :] cluster, const double[::1] weights):\n",
        "    cdef Py_ssize_t cluster_size = cluster.shape[1], d = cluster.shape[0], i, j\n",
        "    cdef double[::1] cluster_mean = np.empty(d)\n",
        "    cdef double weights_sum = get_vector_sum(weights)\n",
        "\n",
        "    for i in prange(d, nogil=True, schedule=\"static\"):\n",
        "        cluster_mean[i] = 0\n",
        "        for j in range(cluster_size):\n",
        "            cluster_mean[i] += weights[j] * cluster[i, j]\n",
        "        cluster_mean[i] /= weights_sum\n",
        "\n",
        "    return cluster_mean, weights_sum\n",
        "\n",
        "\n",
        "cdef inline double get_vector_sum(const double[::1] vector) nogil:\n",
        "    cdef double vector_sum = 0.0\n",
        "    cdef Py_ssize_t i, n = vector.shape[0]\n",
        "\n",
        "    for i in prange(n, schedule=\"static\"):\n",
        "        vector_sum += vector[i]\n",
        "\n",
        "    return vector_sum\n",
        "\n",
        "def caratheodory_set_python(np.ndarray[double, ndim=2] points,\n",
        "                            np.ndarray[double, ndim=1, mode='c'] weights):\n",
        "    \"\"\"\n",
        "    This method computes Caratheodory subset for the columns of a :math:`dxn` Matrix, with given weights.\n",
        "\n",
        "    Args:\n",
        "        points(Matrix): A :math:`dxn` Matrix.\n",
        "        weights(ColumnVector): A weights for the columns of :math:`A`.\n",
        "\n",
        "    Returns:\n",
        "        A Caratheodory subset of :math:`d^{2]+1` columns of :math:`A, as a :math:`dx(d^{2}+1)` Matrix,\n",
        "        and their weights as a ColumnVector.\n",
        "    \"\"\"\n",
        "    cdef Py_ssize_t dim = len(points)  # The dimension of all rows, or rows of the matrix, d.\n",
        "    cdef Py_ssize_t left_args = points.shape[1], i, j, n = points.shape[1], start_index\n",
        "    cdef double alpha, temp_div, v_sum\n",
        "    cdef np.ndarray[double, ndim=2] diff_points\n",
        "    cdef np.ndarray[double, ndim=1] v\n",
        "    cdef np.ndarray left_indices = np.arange(0, n, dtype=int)\n",
        "\n",
        "    while left_args > dim + 1:\n",
        "        diff_points = points[:, left_indices]\n",
        "        diff_points = (diff_points[:, 1:].T - diff_points[:, 0].T).T\n",
        "\n",
        "        v = np.linalg.svd(diff_points, full_matrices=True)[2][left_args - 2]\n",
        "\n",
        "        v_sum = get_vector_sum(v)\n",
        "        v = np.insert(v, [0], -v_sum)\n",
        "\n",
        "        alpha = INFINITY\n",
        "        for i in range(left_args):\n",
        "            if v[i] > 0:\n",
        "                temp_div = weights[left_indices[i]] / v[i]\n",
        "                if temp_div < alpha:\n",
        "                    alpha = temp_div\n",
        "\n",
        "        weights[left_indices] -= alpha * v\n",
        "        left_indices = np.nonzero(weights > 1e-15)[0]\n",
        "        left_args = len(left_indices)\n",
        "\n",
        "    return np.array(weights)[left_indices], left_indices\n",
        "\n",
        "\n",
        "print(\"Loaded Cython Caratheodory Set successfully!\")\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Cython Caratheodory Set successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC-qzZmYWEzF",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "56c9709c-a507-4f5a-9133-0b86b858b331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Solvers Boosters - Caratheodory and Cholesky boosters\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "method_boosters.py - Boosters for least-square solvers\n",
        "======================================================\n",
        "\n",
        "This module contains all available boosters for least-square methods, the Cholesky-booster and the Caratheodory-booster.\n",
        "See the following example of using any booster, i.e cholesky-booster.\n",
        "\n",
        "Example:\n",
        "    boosted_solver = cholesky_booster(existing_solver)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import cholesky\n",
        "\n",
        "_cholesky_covariance: Callable = lambda all_data_mat, clusters_count: cholesky(all_data_mat.T.dot(all_data_mat)).T\n",
        "\n",
        "\n",
        "@ex.capture\n",
        "def _LMS_coreset(all_data_mat: Matrix, cross_validation_folds: int, clusters_count: int) -> Matrix:\n",
        "    sub_matrices: List[Matrix] = np.array_split(all_data_mat, cross_validation_folds)\n",
        "    coreset: Matrix = np.vstack([create_coreset_fast_caratheodory(sub_mat, clusters_count) for sub_mat in sub_matrices])\n",
        "    return coreset\n",
        "\n",
        "\n",
        "def __booster_template(coreset_action: Callable, booster_name: str = \"\") -> Callable:\n",
        "    def __booster(existing_regression_method: Callable, perform_normalization: bool = False) -> Callable:\n",
        "\n",
        "        class _BoostedSolver(BaseSolver):\n",
        "            @ex.capture\n",
        "            def __init__(self, data_features: Matrix, output_samples: ColumnVector, n_alphas: int,\n",
        "                         cross_validation_folds: int, _rnd):\n",
        "                super(_BoostedSolver, self).__init__(data_features, output_samples, n_alphas, cross_validation_folds)\n",
        "                self._inner_model = None\n",
        "                data_length, data_dim = self._data_features.shape\n",
        "                self._beta: Scalar = self._cross_validation_folds * (data_dim + 1) ** 2 + self._cross_validation_folds\n",
        "                self._beta = np.sqrt(self._beta / data_length)\n",
        "                self._perform_normalization = perform_normalization\n",
        "\n",
        "            def fit(self):\n",
        "                all_data_mat: Matrix = np.hstack((self._data_features, self._output_samples.reshape(-1, 1)))\n",
        "                all_data_coreset: Matrix = coreset_action(all_data_mat, self._cross_validation_folds)\n",
        "                L: Matrix = all_data_coreset[:, :-1]\n",
        "                new_output_samples: ColumnVector = all_data_coreset[:, -1]\n",
        "\n",
        "                if self._perform_normalization:\n",
        "                    L *= self._beta\n",
        "                    new_output_samples *= self._beta\n",
        "\n",
        "                self._inner_model = existing_regression_method(L, new_output_samples, self._n_alphas,\n",
        "                                                               self._cross_validation_folds)\n",
        "\n",
        "                self._fitted_coefficients = self._inner_model.fit()\n",
        "                return self._fitted_coefficients\n",
        "\n",
        "        _BoostedSolver.__name__ = f'{booster_name}{existing_regression_method.__name__}'\n",
        "        return _BoostedSolver\n",
        "    return __booster\n",
        "\n",
        "\n",
        "cholesky_booster: Callable = __booster_template(_cholesky_covariance, booster_name=\"Cholesky-boosted\")\n",
        "caratheodory_booster: Callable = __booster_template(_LMS_coreset, booster_name=\"Caratheodory-boosted\")\n",
        "\n",
        "\n",
        "def _greedy_split(arr: Vector, n: int, axis: int = 0, default_block_size: int = 1) -> (List[Vector], Vector):\n",
        "    \"\"\"Greedily splits an array into n blocks.\n",
        "\n",
        "    Splits array arr along axis into n blocks such that:\n",
        "        - blocks 1 through n-1 are all the same size\n",
        "        - the sum of all block sizes is equal to arr.shape[axis]\n",
        "        - the last block is nonempty, and not bigger than the other blocks\n",
        "\n",
        "    Intuitively, this \"greedily\" splits the array along the axis by making\n",
        "    the first blocks as big as possible, then putting the leftovers in the\n",
        "    last block.\n",
        "    \"\"\"\n",
        "    length: int = arr.shape[axis]\n",
        "\n",
        "    # compute the size of each of the first n-1 blocks\n",
        "    if length < n:\n",
        "        n = default_block_size\n",
        "    block_size: int = np.floor(length / float(n))\n",
        "\n",
        "    # the indices at which the splits will occur\n",
        "    ix: Vector = np.arange(block_size, length, block_size, dtype=int)\n",
        "    return np.split(arr, ix, axis), ix\n",
        "\n",
        "\n",
        "def fast_caratheodory_set_python(points: Matrix, weights: ColumnVector, accuracy_time_tradeoff_const: int,\n",
        "                                 cluster_inner_indices=None) -> (Matrix, ColumnVector):\n",
        "    \"\"\"\n",
        "    This method computes Caratheodory subset for the columns of a :math:`dxn` Matrix, with given weights, using the\n",
        "    fast Caratheodory subset algorithm with a given number of clusters.\n",
        "\n",
        "    Args:\n",
        "        points(Matrix): A :math:`dxn` Matrix.\n",
        "        weights(ColumnVector): A weights for the columns of :math:`A`.\n",
        "        accuracy_time_tradeoff_const(int): The number of clusters to use for computing the Caratheodory subset.\n",
        "        cluster_inner_indices:\n",
        "\n",
        "    Returns:\n",
        "        A Caratheodory subset of :math:`d^{2]+1` columns of :math:`A, as a :math:`dx(d^{2}+1)` Matrix,\n",
        "        and their weights as a ColumnVector.\n",
        "    \"\"\"\n",
        "    points_num: int = points.shape[1]  # The number of points, or columns of the matrix, :math:`n`.\n",
        "    dim: int = len(points)  # The dimension of all rows, or rows of the matrix, :math:`d`.\n",
        "\n",
        "    if points_num <= dim + 1:\n",
        "        returned_indices = cluster_inner_indices\n",
        "        if cluster_inner_indices is None:\n",
        "            returned_indices = [x.tolist() for x in np.arange(points_num)]\n",
        "        return weights, returned_indices\n",
        "\n",
        "    # Split points into :math:`k` clusters and find the weighted means of every cluster.\n",
        "    clusters, split_index = _greedy_split(points, accuracy_time_tradeoff_const, axis=1, default_block_size=dim + 2)\n",
        "    split_weights = np.split(weights, split_index)\n",
        "    if cluster_inner_indices is None:\n",
        "        cluster_inner_indices = np.arange(points_num)\n",
        "    split_indices = np.split(cluster_inner_indices, split_index)\n",
        "    arguments_to_parallel_mean = list(zip(clusters, split_weights))\n",
        "    means_and_clusters_weights = compute_serial(cluster_average, arguments_to_parallel_mean)\n",
        "\n",
        "    clusters_means = np.asfortranarray(np.vstack([result[0] for result in means_and_clusters_weights]))\n",
        "    clusters_weights = np.array([result[1] for result in means_and_clusters_weights])\n",
        "\n",
        "    # Perform caratheodory method on the set of means and the total weights of each cluster.\n",
        "    coreset_weights, chosen_indices = caratheodory_set_python(clusters_means.T, clusters_weights)\n",
        "\n",
        "    C = []\n",
        "    c_weights = []\n",
        "    all_chosen_indices = []\n",
        "    # Take C as the union of rows from clusters which the caratheodory set consists of.\n",
        "    for cluster_total_weight, cluster_index in zip(coreset_weights, chosen_indices):\n",
        "        cluster = clusters[cluster_index]\n",
        "        cluster_weights = split_weights[cluster_index]\n",
        "        C.append(cluster)\n",
        "        all_chosen_indices += np.ravel(split_indices[cluster_index]).tolist()\n",
        "        c_weights.append(cluster_total_weight * cluster_weights / np.sum(cluster_weights))\n",
        "\n",
        "    # Assign new weights for each row in C.\n",
        "    # Perform fast caratheodory method recursively.\n",
        "    C = np.hstack(C)\n",
        "    c_weights = np.hstack(c_weights)\n",
        "    new_weights, all_chosen_indices = fast_caratheodory_set_python(C, c_weights, accuracy_time_tradeoff_const,\n",
        "                                                                   all_chosen_indices)\n",
        "    return new_weights, all_chosen_indices\n",
        "\n",
        "\n",
        "def create_coreset_fast_caratheodory(A: Matrix, clusters_count: int) -> Matrix:\n",
        "    \"\"\"\n",
        "    This method computes the outer-products of the rows of the input matrix. Then it computes a coreset for it,\n",
        "    using :func:`fast_caratheodory_set_python` with ``clusters_count`` clusters.\n",
        "\n",
        "    Args:\n",
        "        A(Matrix): An input :math:nxd matrix.\n",
        "        clusters_count(int): The number of clusters to use for computing the coreset.\n",
        "\n",
        "    Returns:\n",
        "        A :math:`(d^{2}+1)xd` Matrix coreset for the input matrix.\n",
        "    \"\"\"\n",
        "    n: int = A.shape[0]  # The number of points, or rows of the matrix, n.\n",
        "\n",
        "    rows_outer_products: Matrix = np.einsum(\"ij,ik->ijk\", A, A, optimize=True)\n",
        "    rows_outer_products = rows_outer_products.reshape((n, -1)).T\n",
        "    weights, remaining_indices = fast_caratheodory_set_python(rows_outer_products, np.ones(n) / n, clusters_count)\n",
        "    reduced_mat: Matrix = np.multiply(A[remaining_indices, :].T, np.sqrt(n * weights)).T\n",
        "    return reduced_mat\n",
        "\n",
        "print(\"Solvers Boosters loaded successfully!\")\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Solvers Boosters loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F65Djil_ZC34",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "d32ebd78-aafe-43f6-ba31-8b4ed3aed030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Linear-Regression Solvers\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "linear_regression.py - Linear-Regression solvers module\n",
        "=======================================================\n",
        "\n",
        "This module contains all available Linear-Regression solvers.\n",
        "These solvers can be received by using the only public method :func:`get_method`.\n",
        "\n",
        "Example:\n",
        "    get_linear_regression_method(LinearRegressionMethods.SVDBased) - \n",
        "      Creating the standard Numpy solver for Linear-Regression.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from numpy.linalg import lstsq, inv, pinv, qr\n",
        "from scipy.linalg import solve_triangular\n",
        "from scipy.sparse.linalg import lsqr\n",
        "\n",
        "\n",
        "class _SVDSolver(BaseSolver):\n",
        "    @ex.capture\n",
        "    def __init__(self, data_features: Matrix, output_samples: ColumnVector, cross_validation_folds: int,\n",
        "                 n_alphas: int = -1):\n",
        "        r\"\"\"\n",
        "        The standard solver of Numpy for Linear-Regression.\n",
        "\n",
        "        Args:\n",
        "            data_features(Matrix): The input data matrix :math:`n \\times d`.\n",
        "            output_samples(ColumnVector): The output for the given inputs, :math:`n \\times 1`.\n",
        "            n_alphas(int): The number of total regularization terms which will be tested by this solver.\n",
        "            cross_validation_folds(int): The number of cross-validation folds used in this solver.\n",
        "\n",
        "        \"\"\"\n",
        "        super(_SVDSolver, self).__init__(data_features, output_samples, n_alphas, cross_validation_folds)\n",
        "        self._model = None\n",
        "\n",
        "    def fit(self) -> ColumnVector:\n",
        "        \"\"\"\n",
        "        The method which fits the requested model to the given data.\n",
        "        \"\"\"\n",
        "        self._fitted_coefficients = lstsq(self._data_features, self._output_samples, rcond=-1)[0]\n",
        "        return self._fitted_coefficients\n",
        "\n",
        "\n",
        "class _QRSolver(BaseSolver):\n",
        "    @ex.capture\n",
        "    def __init__(self, data_features: Matrix, output_samples: ColumnVector, cross_validation_folds: int,\n",
        "                 n_alphas: int = -1):\n",
        "        r\"\"\"\n",
        "        A solver for Linear-Regression, based on QR-decomposition.\n",
        "\n",
        "        Args:\n",
        "            data_features(Matrix): The input data matrix :math:`n \\times d`.\n",
        "            output_samples(ColumnVector): The output for the given inputs, :math:`n \\times 1`.\n",
        "            n_alphas(int): The number of total regularization terms which will be tested by this solver.\n",
        "            cross_validation_folds(int): The number of cross-validation folds used in this solver.\n",
        "\n",
        "        \"\"\"\n",
        "        super(_QRSolver, self).__init__(data_features, output_samples, n_alphas, cross_validation_folds)\n",
        "        self._model = None\n",
        "\n",
        "    def fit(self) -> ColumnVector:\n",
        "        \"\"\"\n",
        "        The method which fits the requested model to the given data.\n",
        "        \"\"\"\n",
        "        q, r = qr(self._data_features)\n",
        "        self._fitted_coefficients = solve_triangular(r, q.T.dot(self._output_samples), lower=False, check_finite=False)\n",
        "        return self._fitted_coefficients\n",
        "\n",
        "\n",
        "class _NormalEquationsSolver(BaseSolver):\n",
        "    @ex.capture\n",
        "    def __init__(self, data_features: Matrix, output_samples: ColumnVector, cross_validation_folds: int,\n",
        "                 n_alphas: int = -1):\n",
        "        r\"\"\"\n",
        "        A solver for Linear-Regression, based on solving the Normal-Equations.\n",
        "\n",
        "        Args:\n",
        "            data_features(Matrix): The input data matrix :math:`n \\times d`.\n",
        "            output_samples(ColumnVector): The output for the given inputs, :math:`n \\times 1`.\n",
        "            n_alphas(int): The number of total regularization terms which will be tested by this solver.\n",
        "            cross_validation_folds(int): The number of cross-validation folds used in this solver.\n",
        "\n",
        "        \"\"\"\n",
        "        super(_NormalEquationsSolver, self).__init__(data_features, output_samples, n_alphas, cross_validation_folds)\n",
        "        self._model = None\n",
        "\n",
        "    def fit(self) -> ColumnVector:\n",
        "        \"\"\"\n",
        "        The method which fits the requested model to the given data.\n",
        "        \"\"\"\n",
        "        self._fitted_coefficients = pinv(self._data_features).dot(self._output_samples)\n",
        "        return self._fitted_coefficients\n",
        "\n",
        "\n",
        "class _SketchPreconditioerSolver(BaseSolver):\n",
        "    @ex.capture\n",
        "    def __init__(self, data_features: Matrix, output_samples: ColumnVector, cross_validation_folds: int, _seed,\n",
        "                 n_alphas: int = -1):\n",
        "        r\"\"\"\n",
        "        A solver for Linear-Regression, based on solving the Normal-Equations.\n",
        "\n",
        "        Args:\n",
        "            data_features(Matrix): The input data matrix :math:`n \\times d`.\n",
        "            output_samples(ColumnVector): The output for the given inputs, :math:`n \\times 1`.\n",
        "            n_alphas(int): The number of total regularization terms which will be tested by this solver.\n",
        "            cross_validation_folds(int): The number of cross-validation folds used in this solver.\n",
        "\n",
        "        \"\"\"\n",
        "        super(_SketchPreconditioerSolver, self).__init__(data_features, output_samples, n_alphas,\n",
        "                                                         cross_validation_folds)\n",
        "        self._model = None\n",
        "        self._seed = _seed\n",
        "\n",
        "    @ex.capture(prefix=\"sketch_preconditioned_config\")\n",
        "    def fit(self, sampled_rows: float, switch_sign_probability: float, min_sampled_rows: float) -> ColumnVector:\n",
        "        \"\"\"\n",
        "        The method which fits the requested model to the given data.\n",
        "        \"\"\"\n",
        "        num_of_rows: int = max(int(sampled_rows * len(self._data_features)), int(min_sampled_rows))\n",
        "        _, R = qr(generate_sketch_preconditioner(self._data_features, num_of_rows, np.empty_like(self._data_features),\n",
        "                                                 self._seed, switch_sign_probability))\n",
        "        partial_solution: ColumnVector = lsqr(self._data_features.dot(inv(R)), self._output_samples,\n",
        "                                              atol=1e-15, btol=1e-15)[0]\n",
        "        self._fitted_coefficients = solve_triangular(R, partial_solution, lower=False, check_finite=False)\n",
        "        return self._fitted_coefficients\n",
        "\n",
        "\n",
        "class _SketchInverseSolver(BaseSolver):\n",
        "    @ex.capture\n",
        "    def __init__(self, data_features: Matrix, output_samples: ColumnVector, cross_validation_folds: int,\n",
        "                 n_alphas: int = -1):\n",
        "        r\"\"\"\n",
        "        A solver for Linear-Regression, based on boosting the algorithm which solves the Normal-Equations,\n",
        "        using fast Caratheodory method.\n",
        "\n",
        "        Args:\n",
        "            data_features(Matrix): The input data matrix :math:`n \\times d`.\n",
        "            output_samples(ColumnVector): The output for the given inputs, :math:`n \\times 1`.\n",
        "            n_alphas(int): The number of total regularization terms which will be tested by this solver.\n",
        "            cross_validation_folds(int): The number of cross-validation folds used in this solver.\n",
        "\n",
        "        \"\"\"\n",
        "        super(_SketchInverseSolver, self).__init__(data_features, output_samples, -1, cross_validation_folds)\n",
        "        self._model = None\n",
        "\n",
        "    @ex.capture\n",
        "    def fit(self, clusters_count) -> ColumnVector:\n",
        "        \"\"\"\n",
        "        The method which fits the requested model to the given data.\n",
        "        \"\"\"\n",
        "        coreset: Matrix = create_coreset_fast_caratheodory(self._data_features, clusters_count)\n",
        "        adapted_data, adapted_output, outputs_sum = self._preprocess_data()\n",
        "        weights, chosen_indices = fast_caratheodory_set_python(adapted_data.T, adapted_output, clusters_count)\n",
        "        a_times_outputs: ColumnVector = outputs_sum * adapted_data[chosen_indices, :].T.dot(weights)\n",
        "        self._fitted_coefficients = inv(coreset.T.dot(coreset)).dot(a_times_outputs)\n",
        "        return self._fitted_coefficients\n",
        "\n",
        "    def _preprocess_data(self):\n",
        "        data_copy: Matrix = self._data_features.copy()\n",
        "        output_copy: ColumnVector = self._output_samples.copy()\n",
        "        negative_indices: ColumnVector = np.argwhere(self._output_samples < 0)\n",
        "        data_copy[negative_indices, :] *= -1\n",
        "        output_copy[negative_indices] *= -1\n",
        "        output_sum = np.sum(output_copy)\n",
        "        return data_copy, output_copy/output_sum, output_sum\n",
        "\n",
        "\n",
        "_sketch_cholesky_linear_regression: Callable = cholesky_booster(_SVDSolver)\n",
        "_caratheodory_booster_linear_regression: Callable = caratheodory_booster(_SVDSolver, perform_normalization=False)\n",
        "\n",
        "# A private dictionary used for creating the solvers factory :func:`get_method`.\n",
        "_linear_regressions_methods: Dict[str, Callable] = {\n",
        "    LinearRegressionMethods.SVDBased: _SVDSolver,\n",
        "    LinearRegressionMethods.QRBased: _QRSolver,\n",
        "    LinearRegressionMethods.NormalEquationsBased: _NormalEquationsSolver,\n",
        "    LinearRegressionMethods.SketchAndCholesky: _sketch_cholesky_linear_regression,\n",
        "    LinearRegressionMethods.BoostedSVDSolver: _caratheodory_booster_linear_regression,\n",
        "    LinearRegressionMethods.SketchAndInverse: _SketchInverseSolver,\n",
        "    LinearRegressionMethods.SketchPreconditioned: _SketchPreconditioerSolver\n",
        "}\n",
        "\n",
        "# A factory which creates the requested linear-regression solvers.\n",
        "get_linear_regression_method: Callable = create_factory(_linear_regressions_methods, are_methods=True)\n",
        "\n",
        "print(\"Linear-Regression Methods loaded successfully!\")\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear-Regression Methods loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxRETo48bLsN",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "c8344b65-c934-4b16-c23e-acb5249a7f71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Lasso-Regression Solvers\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "lasso_regression.py - Lasso-Regression solvers module\n",
        "=====================================================\n",
        "\n",
        "This module contains all available Lasso-Regression solvers.\n",
        "These solvers can be received by using the only public method :func:`get_method`.\n",
        "\n",
        "Example:\n",
        "    get_lass_regresion_method(LassoRegressionMethods.SkLearnLassoRegression) - \n",
        "      Creating the Scikit-Learn solver for Lasso-Regression.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "\n",
        "class _SkLearnLassoSolver(BaseSolver):\n",
        "    @ex.capture\n",
        "    def __init__(self, data_features: Matrix, output_samples: ColumnVector, n_alphas: int,\n",
        "                 cross_validation_folds: int, _rnd):\n",
        "        \"\"\"\n",
        "        The standard solver of Scikit-Learn for Lasso-Regression.\n",
        "\n",
        "        Args:\n",
        "            data_features(Matrix): The input data matrix ``nxd``.\n",
        "            output_samples(ColumnVector): The output for the given inputs, ``nx1``.\n",
        "            n_alphas(int): The number of total regularization terms which will be tested by this solver.\n",
        "            cross_validation_folds(int): The number of cross-validation folds used in this solver.\n",
        "\n",
        "        \"\"\"\n",
        "        super(_SkLearnLassoSolver, self).__init__(data_features, output_samples, n_alphas, cross_validation_folds)\n",
        "        self._model = LassoCV(cv=cross_validation_folds, n_alphas=n_alphas, \n",
        "                              random_state=_rnd, normalize=False)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        The method which fits the requested model to the gicen data.\n",
        "        \"\"\"\n",
        "        self._model.fit(self._data_features, self._output_samples)\n",
        "        self._fitted_coefficients = self._model.coef_\n",
        "        return self._fitted_coefficients\n",
        "\n",
        "\n",
        "_caratheodory_boosted_lasso_regression: Callable = caratheodory_booster(_SkLearnLassoSolver,\n",
        "                                                                        perform_normalization=True)\n",
        "\n",
        "# A private dictionary used for creating the solvers factory :func:`get_method`.\n",
        "_lasso_regressions_methods: Dict[str, Callable] = {\n",
        "    LassoRegressionMethods.SkLearnLassoRegression: _SkLearnLassoSolver,\n",
        "    LassoRegressionMethods.BoostedLassoRegression: _caratheodory_boosted_lasso_regression\n",
        "}\n",
        "\n",
        "# A factory which creates the requested Lasso-Regression solvers.\n",
        "get_lasso_regression_method: Callable = create_factory(_lasso_regressions_methods, are_methods=True)\n",
        "print(\"Lasso-Regression Methods loaded successfully!\")\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lasso-Regression Methods loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubwLBl3QbL6b",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "349ed31e-1c10-4a40-a19c-1f1eb9b7e9dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Ridge-Regression Solvers\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "ridge_regression.py - Ridge-Regression solvers module\n",
        "=====================================================\n",
        "\n",
        "This module contains all available Ridge-Regression solvers.\n",
        "These solvers can be received by using the only public method :func:`get_method`.\n",
        "\n",
        "Example:\n",
        "    get_ridge_regreesion_method(RidgeRegressionMethods.SkLearnLassoRegression) - \n",
        "      Creating the Scikit-Learn solver for Ridge-Regression.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "\n",
        "class _SkLearnRidgeSolver(BaseSolver):\n",
        "    @ex.capture\n",
        "    def __init__(self, data_features: Matrix, output_samples: ColumnVector, n_alphas: int,\n",
        "                 cross_validation_folds: int):\n",
        "        \"\"\"\n",
        "        The standard solver of Scikit-Learn for Lasso-Regression.\n",
        "\n",
        "        Args:\n",
        "            data_features(Matrix): The input data matrix ``nxd``.\n",
        "            output_samples(ColumnVector): The output for the given inputs, ``nx1``.\n",
        "            n_alphas(int): The number of total regularization terms which will be tested by this solver.\n",
        "            cross_validation_folds(int): The number of cross-validation folds used in this solver.\n",
        "\n",
        "        \"\"\"\n",
        "        super(_SkLearnRidgeSolver, self).__init__(data_features, output_samples, n_alphas, cross_validation_folds)\n",
        "        alphas: ColumnVector = np.random.randn(n_alphas)\n",
        "        self._model = RidgeCV(cv=cross_validation_folds, alphas=alphas, normalize=False)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        The method which fits the requested model to the gicen data.\n",
        "        \"\"\"\n",
        "        self._model.fit(self._data_features, self._output_samples)\n",
        "        self._fitted_coefficients = self._model.coef_\n",
        "        return self._fitted_coefficients\n",
        "\n",
        "\n",
        "_caratheodory_boosted_ridge_regression: Callable = caratheodory_booster(_SkLearnRidgeSolver,\n",
        "                                                                        perform_normalization=False)\n",
        "\n",
        "# A private dictionary used for creating the solvers factory :func:`get_method`.\n",
        "_ridge_regressions_methods: Dict[str, Callable] = {\n",
        "    RidgeRegressionMethods.SkLearnRidgeRegression: _SkLearnRidgeSolver,\n",
        "    RidgeRegressionMethods.BoostedRidgeRegression: _caratheodory_boosted_ridge_regression\n",
        "}\n",
        "\n",
        "# A factory which creates the requested Ridge-Regression solvers.\n",
        "get_ridge_regression_method: Callable = create_factory(_ridge_regressions_methods, are_methods=True)\n",
        "\n",
        "print(\"Ridge-Regression Methods loaded successfully!\")\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ridge-Regression Methods loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDAqAxirbMAS",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "ed126dca-3d55-41de-d2b8-92883a5ef75a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title ElasticNet-Regression Solvers\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "elastic_net_regressions.py - ElasticNet-Regression solvers module\n",
        "=================================================================\n",
        "\n",
        "This module contains all available ElasticNet-Regression solvers.\n",
        "These solvers can be received by using the only public method :func:`get_method`.\n",
        "\n",
        "Example:\n",
        "    get_relasticnet_regression-method(ElasticNetRegressionMethods.SkLearnLassoRegression) \n",
        "      - Creating the Scikit-Learn solver for ElasticNet-Regression.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "\n",
        "class _SkLearnElasticNetSolver(BaseSolver):\n",
        "    @ex.capture\n",
        "    def __init__(self, data_features: Matrix, output_samples: ColumnVector, n_alphas: int,\n",
        "                 cross_validation_folds: int, elastic_net_factor: Scalar, _rnd):\n",
        "        \"\"\"\n",
        "        The standard solver of Scikit-Learn for Lasso-Regression.\n",
        "\n",
        "        Args:\n",
        "            data_features(Matrix): The input data matrix ``nxd``.\n",
        "            output_samples(ColumnVector): The output for the given inputs, ``nx1``.\n",
        "            n_alphas(int): The number of total regularization terms which will be tested by this solver.\n",
        "            cross_validation_folds(int): The number of cross-validation folds used in this solver.\n",
        "\n",
        "        \"\"\"\n",
        "        super(_SkLearnElasticNetSolver, self).__init__(data_features, output_samples, n_alphas, cross_validation_folds)\n",
        "        self._model = ElasticNetCV(cv=cross_validation_folds, n_alphas=n_alphas, random_state=_rnd,\n",
        "                                   l1_ratio=elastic_net_factor, normalize=False)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        The method which fits the requested model to the gicen data.\n",
        "        \"\"\"\n",
        "        self._model.fit(self._data_features, self._output_samples)\n",
        "        self._fitted_coefficients = self._model.coef_\n",
        "        return self._fitted_coefficients\n",
        "\n",
        "_caratheodory_boosted_elastic_net_regression: Callable = caratheodory_booster(_SkLearnElasticNetSolver,\n",
        "                                                                              perform_normalization=True)\n",
        "\n",
        "# A private dictionary used for creating the solvers factory 'get_method'.\n",
        "_elastic_net_regressions_methods: Dict[str, Callable] = {\n",
        "    ElasticNetRegressionMethods.SkLearnElasticNetRegression: _SkLearnElasticNetSolver,\n",
        "    ElasticNetRegressionMethods.BoostedElasticNetRegression: _caratheodory_boosted_elastic_net_regression\n",
        "}\n",
        "\n",
        "\n",
        "# A factory which creates the relevant Elastic-Net-Regression solver.\n",
        "get_elasticnet_regression_method: Callable = create_factory(_elastic_net_regressions_methods, are_methods=True)\n",
        "\n",
        "print(\"ElasticNet-Regression Methods loaded successfully!\")\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ElasticNet-Regression Methods loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6U6m8SgV8cC",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "59fd1535-a96c-487f-a887-e4c8f267d896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title __init__\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "__init__.py - A solvers' factory method\n",
        "========================================\n",
        "\n",
        "This module contains the factory method :func:`get_method` which creates the requested solvers for an experiment.\n",
        "See the following examples on creating solvers.\n",
        "\n",
        "Examples:\n",
        "    get_methods(AlgorithmsType.LinearRegression, []) - Creates all available solvers for Linear-Regression.\n",
        "    get_methods(AlgorithmsType.LinearRegression, [LinearRegressionMethods.SVDBased]) - Creates only the standard Numpy\n",
        "     solver for Linear-Regression.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# A private dictionary used for get_method.\n",
        "_algorithms_type_to_algorithms: Dict = {\n",
        "    AlgorithmsType.LinearRegression: (LinearRegressionMethods, get_linear_regression_method),\n",
        "    AlgorithmsType.RidgeRegression: (RidgeRegressionMethods, get_ridge_regression_method),\n",
        "    AlgorithmsType.LassoRegression: (LassoRegressionMethods, get_lasso_regression_method),\n",
        "    AlgorithmsType.ElasticNetRegression: (ElasticNetRegressionMethods, get_elasticnet_regression_method)\n",
        "}\n",
        "\n",
        "\n",
        "def get_methods(requested_algorithms_type: AlgorithmsType, compared_methods: List) -> List:\n",
        "    \"\"\"\n",
        "    A factory which creates the requested solvers.\n",
        "\n",
        "    Args:\n",
        "        requested_algorithms_type(AlgorithmsType): The name of the solvers type, i.e linear-regression\n",
        "        compared_methods(List): A list of specific solvers to create. If empty, all solvers of a given type are created.\n",
        "\n",
        "    Returns:\n",
        "         A list of all relevant solvers.\n",
        "\n",
        "    \"\"\"\n",
        "    solvers_names_list, solvers_factory = _algorithms_type_to_algorithms[requested_algorithms_type]\n",
        "    solvers: List = list()\n",
        "    if is_empty(compared_methods):\n",
        "        for solver_name in solvers_names_list:\n",
        "            solvers.append(solvers_factory(solver_name))\n",
        "    else:\n",
        "        for solver_name in compared_methods:\n",
        "            solvers.append(solvers_factory(solver_name))\n",
        "    return solvers\n",
        "\n",
        "print(\"ComparedAlgorithms.__init__ loaded successfully!\")\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ComparedAlgorithms.__init__ loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o79QnJ_1dQ6t",
        "colab_type": "text"
      },
      "source": [
        "#Main Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeUlmtf2deRL",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "c658b56c-fe84-46ad-afd8-c635fcb443e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Database loader\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "database_loader.py - The data management module\n",
        "===============================================\n",
        "\n",
        "This module handles the fetching of the data from the local resources path, given in the configuration and arranging it\n",
        "for our purposes of estimations. See the example for fetching the 3D Road Network database.\n",
        "\n",
        "Example:\n",
        "    get_data(DatabaseType.ThreeDRoadNetwork) - Creating the standard Numpy solver for Linear-Regression.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from numpy.random import randn\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "@ex.capture(prefix=\"synthetic_data_config\")\n",
        "def _get_synthetic_data(data_size: int, features_num: int) -> (Matrix, ColumnVector):\n",
        "    \"\"\"\n",
        "    A method which creates a random matrix of ``size data_size x features_num`` and a ``random 1 x data_size``\n",
        "    random vector.\n",
        "\n",
        "    Args:\n",
        "        data_size(int): The number of samples in the data - ``n``.\n",
        "        features_num(int): The number of columns in the data - ``d``.\n",
        "\n",
        "    Returns:\n",
        "        A random ``size data_size x features_num`` Matrix and a random ``1 x data_size`` ColumnVector.\n",
        "\n",
        "    \"\"\"\n",
        "    return randn(data_size, features_num), randn(data_size)\n",
        "\n",
        "\n",
        "@ex.capture\n",
        "def _get_3d_road_network_data(resources_path: str) -> (Matrix, ColumnVector):\n",
        "    \"\"\"\n",
        "    A method which loads the 3D Road Network database from the given local path.\n",
        "    See https://archive.ics.uci.edu/ml/datasets/3D+Road+Network+(North+Jutland,+Denmark)\n",
        "\n",
        "    Args:\n",
        "        resources_path(str): The path in which this database is stored in.\n",
        "\n",
        "    Returns:\n",
        "        A Matrix of the features, besides the height, and a ColumnVector of the height feature.\n",
        "\n",
        "    \"\"\"\n",
        "    data: Matrix = scale(np.loadtxt(os.path.join(resources_path, \"3D_spatial_network.txt\"), delimiter=','))\n",
        "    output_samples: ColumnVector = data[:, -1]\n",
        "    data_matrix: Matrix = np.ascontiguousarray(data[:, 1:3])\n",
        "    return data_matrix, output_samples\n",
        "\n",
        "\n",
        "@ex.capture\n",
        "def _get_house_sales_in_king_county_data(resources_path: str) -> (Matrix, ColumnVector):\n",
        "    \"\"\"\n",
        "    A method which loads the House sales in King County database from the given local path.\n",
        "    See https://www.kaggle.com/harlfoxem/housesalesprediction\n",
        "\n",
        "    Args:\n",
        "        resources_path(str): The path in which this database is stored in.\n",
        "\n",
        "    Returns:\n",
        "        A Matrix of the features, besides the price, and a ColumnVector of the price feature.\n",
        "\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(os.path.join(resources_path, \"kc_house_data.csv\"))\n",
        "    output_samples: ColumnVector = scale(df[\"price\"].to_numpy())\n",
        "    data_matrix: Matrix = np.ascontiguousarray(scale(df[[\"bedrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\",\n",
        "                                                         \"sqft_above\", \"sqft_basement\", \"yr_built\"]].to_numpy()))\n",
        "    return data_matrix, output_samples\n",
        "\n",
        "\n",
        "@ex.capture\n",
        "def _get_household_electric_power_consumption_data(resources_path: str) -> (Matrix, ColumnVector):\n",
        "    \"\"\"\n",
        "    A method which loads the Household electric power consumption database from the given local path.\n",
        "    See https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption.\n",
        "\n",
        "    Args:\n",
        "        resources_path(str): The path in which this database is stored in.\n",
        "\n",
        "    Returns:\n",
        "        A Matrix of the features, besides the voltage, and a ColumnVector of the voltage feature.\n",
        "\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(os.path.join(resources_path, \"household_power_consumption.txt\"), sep=';', na_values=\"?\")\n",
        "    df.dropna(axis=0, inplace=True)\n",
        "    output_samples: ColumnVector = scale(pd.to_numeric(df[\"Voltage\"]).to_numpy())\n",
        "    data_matrix: Matrix = np.ascontiguousarray(scale(df[[\"Global_active_power\", \"Global_reactive_power\"]].astype(\"float64\").to_numpy()))\n",
        "    return data_matrix, output_samples\n",
        "\n",
        "\n",
        "# A private dictionary used to create the method \"get_data\"\n",
        "_database_type_to_function: Dict[str, Callable] = {\n",
        "    DatabaseType.Synthetic: _get_synthetic_data,\n",
        "    DatabaseType.ThreeDRoadNetwork: _get_3d_road_network_data,\n",
        "    DatabaseType.HouseSalesInKingCounty: _get_house_sales_in_king_county_data,\n",
        "    DatabaseType.IndividualHouseholdElectricPowerConsumption: _get_household_electric_power_consumption_data\n",
        "}\n",
        "\n",
        "# The public method which fetches the data loading methods.\n",
        "get_data: Callable = create_factory(_database_type_to_function)\n",
        "\n",
        "print(\"Database loader loaded successfully!\")\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Database loader loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGltcOrvd4cw",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "83e5ca37-2514-450c-b834-4465dc14dfcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Experiments\n",
        "from numpy.linalg import norm\n",
        "\n",
        "@ex.capture()\n",
        "def _run_time_experiment(compared_solvers: List, data_matrix: Matrix, output_samples: Vector,\n",
        "                         run_time_experiments_config):\n",
        "    all_logs: Dict = dict()\n",
        "    run_time_compared_data_sizes: List = run_time_experiments_config[\"run_time_compared_data_sizes\"]\n",
        "    calc_transpose_dot_residuals: bool = run_time_experiments_config[\"calc_transpose_dot_residuals\"]\n",
        "\n",
        "    for solver in compared_solvers:\n",
        "        error_raised: bool = False\n",
        "        log_fields: List = [LogFields.DataSize, LogFields.Coefficients, LogFields.Residuals,\n",
        "                            LogFields.DurationInSeconds]\n",
        "        if calc_transpose_dot_residuals:\n",
        "            log_fields.append(LogFields.AtTimesErrors)\n",
        "\n",
        "        data_log = DataLog(log_fields)\n",
        "        coefficients_list = list()\n",
        "        residuals_list = list()\n",
        "        transpose_errors_list = list()\n",
        "        durations_list = list()\n",
        "\n",
        "        for data_size in run_time_compared_data_sizes:\n",
        "            # try:\n",
        "            partial_data: Matrix = data_matrix[:data_size, :]\n",
        "            partial_samples: Vector = output_samples[:data_size]\n",
        "            solver_obj = solver(partial_data, partial_samples)\n",
        "            coefficients, duration = measure_time(solver_obj.fit)()\n",
        "            residuals: Vector = solver_obj.calc_residuals()\n",
        "\n",
        "            coefficients_list.append(coefficients.tolist())\n",
        "            residuals_list.append(norm(residuals))\n",
        "            if calc_transpose_dot_residuals:\n",
        "                transpose_errors_list.append(norm(partial_data.T.dot(residuals)))\n",
        "            durations_list.append(duration)\n",
        "            print(f'solver name={solver.__name__}, data size={data_size}, duration={duration}')\n",
        "\n",
        "            # except IOError:\n",
        "            #     error_raised = True\n",
        "            #     continue\n",
        "\n",
        "        if not error_raised:\n",
        "            data_log.append(LogFields.DataSize, list(run_time_compared_data_sizes))\n",
        "            data_log.append(LogFields.Residuals, residuals_list)\n",
        "            if calc_transpose_dot_residuals:\n",
        "                data_log.append(LogFields.AtTimesErrors, transpose_errors_list)\n",
        "            data_log.append(LogFields.Coefficients, coefficients_list)\n",
        "            data_log.append(LogFields.DurationInSeconds, durations_list)\n",
        "            all_logs[solver.__name__] = data_log\n",
        "\n",
        "    return all_logs\n",
        "\n",
        "\n",
        "@ex.capture(prefix=\"number_of_alphas_experiments_config\")\n",
        "def _number_of_alphas_experiment(compared_solvers: List, data_matrix: Matrix, output_samples: Vector,\n",
        "                                 alphas_range: List):\n",
        "    all_logs: Dict = dict()\n",
        "    for solver in compared_solvers:\n",
        "        error_raised: bool = False\n",
        "        data_log = DataLog([LogFields.Coefficients, LogFields.Residuals, LogFields.DurationInSeconds,\n",
        "                            LogFields.AlphasCount])\n",
        "        coefficients_list = list()\n",
        "        residuals_list = list()\n",
        "        durations_list = list()\n",
        "\n",
        "        for current_alphas in alphas_range:\n",
        "            # try:\n",
        "              solver_obj = solver(data_matrix, output_samples, n_alphas=current_alphas)\n",
        "              coefficients, duration = measure_time(solver_obj.fit)()\n",
        "              residuals: Vector = solver_obj.calc_residuals()\n",
        "\n",
        "              coefficients_list.append(coefficients.tolist())\n",
        "              residuals_list.append(norm(residuals))\n",
        "              durations_list.append(duration)\n",
        "              print(f'solver name={solver.__name__}, total alphas={current_alphas}, duration={duration}')\n",
        "\n",
        "            # except IOError:\n",
        "            #     error_raised = True\n",
        "            #     continue\n",
        "\n",
        "        if not error_raised:\n",
        "            data_log.append(LogFields.AlphasCount, list(alphas_range))\n",
        "            data_log.append(LogFields.Residuals, residuals_list)\n",
        "            data_log.append(LogFields.Coefficients, coefficients_list)\n",
        "            data_log.append(LogFields.DurationInSeconds, durations_list)\n",
        "            all_logs[solver.__name__] = data_log\n",
        "\n",
        "    return all_logs\n",
        "\n",
        "\n",
        "_experiment_type_to_method: Dict = {\n",
        "    ExperimentType.RunTimeExperiment: _run_time_experiment,\n",
        "    ExperimentType.NumberOfAlphasExperiment: _number_of_alphas_experiment\n",
        "}\n",
        "\n",
        "create_experiment: Callable = create_factory(_experiment_type_to_method, are_methods=True)\n",
        "\n",
        "print(\"Experiments loaded successfully!\")\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiments loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvYVsmHteTHQ",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "30196bd9-8d78-4139-8558-0c20949cc587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "#@title Main\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "main.py - The main module of the project\n",
        "========================================\n",
        "\n",
        "This module contains the config for the experiment in the \"config\" function.\n",
        "Running this module invokes the :func:`main` function, which then performs the experiment and saves its results\n",
        "to the configured results folder. Example for running an experiment: ``python main.py``\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def _choose_clusters_num(database_type: str, synthetic_data_dim: int) -> int:\n",
        "    \"\"\"\n",
        "    This method determines the number of clusters for coreset computation, using the number of features in the data.\n",
        "    If the data has :math:`d` features then the number of used clusters is :math:`2(d+1)^{2}+2`.\n",
        "    \"\"\"\n",
        "    data_dim: int = 1\n",
        "    if database_type == DatabaseType.Synthetic:\n",
        "        data_dim = synthetic_data_dim\n",
        "    elif database_type in [DatabaseType.ThreeDRoadNetwork, \n",
        "                           DatabaseType.IndividualHouseholdElectricPowerConsumption]:\n",
        "        data_dim = 2\n",
        "    elif database_type == DatabaseType.HouseSalesInKingCounty:\n",
        "        data_dim = 8\n",
        "    return 2 * (data_dim + 1) ** 2 + 2\n",
        "\n",
        "\n",
        "@ex.config\n",
        "def config():\n",
        "    \"\"\" Config section\n",
        "\n",
        "    This function contains all possible configuration for all experiments. Full details on each configuration values\n",
        "    can be found in :mod:`enums.py`.\n",
        "    \"\"\"\n",
        "    compared_algorithms_type: AlgorithmsType = AlgorithmsType.LinearRegression\n",
        "    compared_methods: List = [LinearRegressionMethods.SketchAndInverse]  # Leave empty for using all solvers.\n",
        "    numpy_distribution: NumpyDistribution = NumpyDistribution.IntelDistribution\n",
        "    used_database: DatabaseType = DatabaseType.HouseSalesInKingCounty\n",
        "    experiment_type: ExperimentType = ExperimentType.RunTimeExperiment\n",
        "    cross_validation_folds: int = 1\n",
        "    \n",
        "    n_alphas: int = 100\n",
        "\n",
        "    run_time_experiments_config: Dict[str, range] = {\n",
        "        \"run_time_compared_data_sizes\": range(500, 22000, 500),\n",
        "        \"calc_transpose_dot_residuals\": compared_algorithms_type == AlgorithmsType.LinearRegression\n",
        "    }\n",
        "    number_of_alphas_experiments_config: Dict[str, range] = {\n",
        "        \"alphas_range\": range(1, 221, 20)\n",
        "    }\n",
        "\n",
        "    synthetic_data_config: Dict[str, int] = {\n",
        "        \"data_size\": 80000000,\n",
        "        \"features_num\": 7\n",
        "    }\n",
        "    sketch_preconditioned_config: Dict[str, float] = {\n",
        "        \"sampled_rows\": 0.005,\n",
        "        \"switch_sign_probability\": 0.5,\n",
        "        \"min_sampled_rows\": 100.0\n",
        "    }\n",
        "    resources_path: str = r'Resources'\n",
        "    results_path: str = r'Results'\n",
        "    clusters_count: int = _choose_clusters_num(used_database, synthetic_data_config[\"features_num\"])\n",
        "    elastic_net_factor: float = 0.5  # Rho factor in Elastic-Net regularization.\n",
        "\n",
        "\n",
        "@ex.main\n",
        "def run_experiment(compared_algorithms_type: AlgorithmsType, compared_methods: List, used_database: DatabaseType,\n",
        "                   experiment_type: ExperimentType, results_path: str) -> Dict[str, DataLog]:\n",
        "    \"\"\" The main function of this project\n",
        "\n",
        "    This functions performs the desired experiment according to the given configuration.\n",
        "    The function then saves all the experiment results to a csv file in the results folder (given in the configuration).\n",
        "    \"\"\"\n",
        "    #sys.setrecursionlimit(1500)\n",
        "    compared_solvers: List = get_methods(compared_algorithms_type, compared_methods)\n",
        "    data_matrix, output_samples = get_data(used_database)\n",
        "    experiment = create_experiment(experiment_type)\n",
        "    all_logs: Dict[str, DataLog] = experiment(compared_solvers, data_matrix, output_samples)\n",
        "\n",
        "    for log_name, log in all_logs.items():\n",
        "        log.save_log(log_name, results_path)\n",
        "\n",
        "    return all_logs\n",
        "\n",
        "ex.run()\n",
        "print(\"Experiment performed Successfully!\")\n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING - Initializing project - No observers have been added to this run\n",
            "INFO - Initializing project - Running command 'run_experiment'\n",
            "INFO - Initializing project - Started\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver name=_SketchInverseSolver, data size=500, duration=0.7006971379996685\n",
            "solver name=_SketchInverseSolver, data size=1000, duration=0.8450672200015106\n",
            "solver name=_SketchInverseSolver, data size=1500, duration=1.071158833001391\n",
            "solver name=_SketchInverseSolver, data size=2000, duration=1.4278948429964657\n",
            "solver name=_SketchInverseSolver, data size=2500, duration=1.8140475059990422\n",
            "solver name=_SketchInverseSolver, data size=3000, duration=1.236588383999333\n",
            "solver name=_SketchInverseSolver, data size=3500, duration=1.3277555169988773\n",
            "solver name=_SketchInverseSolver, data size=4000, duration=1.4726600840003812\n",
            "solver name=_SketchInverseSolver, data size=4500, duration=1.5841118909993384\n",
            "solver name=_SketchInverseSolver, data size=5000, duration=1.8408135739991849\n",
            "solver name=_SketchInverseSolver, data size=5500, duration=2.1761056650029786\n",
            "solver name=_SketchInverseSolver, data size=6000, duration=2.2923864020012843\n",
            "solver name=_SketchInverseSolver, data size=6500, duration=2.1803785730007803\n",
            "solver name=_SketchInverseSolver, data size=7000, duration=1.473958392998611\n",
            "solver name=_SketchInverseSolver, data size=7500, duration=1.5564317889984522\n",
            "solver name=_SketchInverseSolver, data size=8000, duration=1.6790917920006905\n",
            "solver name=_SketchInverseSolver, data size=8500, duration=1.7023970100017323\n",
            "solver name=_SketchInverseSolver, data size=9000, duration=1.6537171879972448\n",
            "solver name=_SketchInverseSolver, data size=9500, duration=1.6138143999996828\n",
            "solver name=_SketchInverseSolver, data size=10000, duration=1.6783590750019357\n",
            "solver name=_SketchInverseSolver, data size=10500, duration=1.7201120280005853\n",
            "solver name=_SketchInverseSolver, data size=11000, duration=1.81547298499936\n",
            "solver name=_SketchInverseSolver, data size=11500, duration=1.8199112160000368\n",
            "solver name=_SketchInverseSolver, data size=12000, duration=1.9227455669970368\n",
            "solver name=_SketchInverseSolver, data size=12500, duration=1.939767389998451\n",
            "solver name=_SketchInverseSolver, data size=13000, duration=2.0035458160018607\n",
            "solver name=_SketchInverseSolver, data size=13500, duration=2.0304904350014112\n",
            "solver name=_SketchInverseSolver, data size=14000, duration=2.3531119270010095\n",
            "solver name=_SketchInverseSolver, data size=14500, duration=2.389189475998137\n",
            "solver name=_SketchInverseSolver, data size=15000, duration=2.4199014980003994\n",
            "solver name=_SketchInverseSolver, data size=15500, duration=2.449669678000646\n",
            "solver name=_SketchInverseSolver, data size=16000, duration=2.4459824850018776\n",
            "solver name=_SketchInverseSolver, data size=16500, duration=2.4562754509970546\n",
            "solver name=_SketchInverseSolver, data size=17000, duration=2.5053394829992612\n",
            "solver name=_SketchInverseSolver, data size=17500, duration=1.9055320730003587\n",
            "solver name=_SketchInverseSolver, data size=18000, duration=1.7557727130006242\n",
            "solver name=_SketchInverseSolver, data size=18500, duration=1.7943190070000128\n",
            "solver name=_SketchInverseSolver, data size=19000, duration=1.7826013510020857\n",
            "solver name=_SketchInverseSolver, data size=19500, duration=1.846176644001389\n",
            "solver name=_SketchInverseSolver, data size=20000, duration=1.8711620370013407\n",
            "solver name=_SketchInverseSolver, data size=20500, duration=1.878652528001112\n",
            "solver name=_SketchInverseSolver, data size=21000, duration=1.871047599997837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO - Initializing project - Result: {'_SketchInverseSolver': <__main__.DataLog object at 0x7fdc318b2fd0>}\n",
            "INFO - Initializing project - Completed after 0:01:18\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "solver name=_SketchInverseSolver, data size=21500, duration=1.947237046999362\n",
            "Experiment performed Successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgc9Wbd9QDOd",
        "colab_type": "text"
      },
      "source": [
        "#Unit-Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvR7adAEobND",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "1501ba5e-adf3-4010-83d6-25f9b9b53139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Testing Caratheodory sets methods\n",
        "import unittest\n",
        "\n",
        "class TestCaratheodorySet(unittest.TestCase):\n",
        "    \"\"\" A class which contains tests for Caratheodory subset computation.\"\"\"\n",
        "\n",
        "    def test_python_Caratheodory_set(self):\n",
        "        \"\"\"\n",
        "        This method tests the basic algorithm for Caratheodory subset algorithm.\n",
        "        \"\"\"\n",
        "        d: int = 15\n",
        "        n: int = 100\n",
        "        data: Matrix = np.random.rand(d, n)\n",
        "        weights: Vector = np.random.random(n)\n",
        "        weights /= weights.sum()\n",
        "        epsilon: Scalar = np.finfo(np.float).eps\n",
        "        new_weights, chosen_indices = caratheodory_set_python(data, weights)\n",
        "        self.assertAlmostEqual(np.sum(weights), 1, delta=5 * epsilon)\n",
        "        self.assertTrue(np.all(weights >= -epsilon), f'\\nSome negative weights: {weights[np.invert(weights >= 0)]}')\n",
        "        self.assertLessEqual(len(chosen_indices), d + 1)\n",
        "        self.assertTrue(np.allclose(data.dot(weights), data[:, chosen_indices].dot(new_weights), atol=1e-10, rtol=0))\n",
        "\n",
        "    def test_fast_python_Caratheodory_set(self):\n",
        "        \"\"\"\n",
        "        This method tests the faster algorithm for Caratheodory subset algorithm.\n",
        "        \"\"\"\n",
        "        d: int = 3\n",
        "        n: int = 50\n",
        "        clusters_count: int = 2 * (d + 1) ** 2 + 2\n",
        "        data: Matrix = np.asfortranarray(np.random.rand(d, n))\n",
        "        weights: Vector = np.random.random(n)\n",
        "        weights /= weights.sum()\n",
        "        epsilon: Scalar = np.finfo(np.float).eps\n",
        "        new_weights, chosen_indices = fast_caratheodory_set_python(data, weights, clusters_count)\n",
        "        self.assertAlmostEqual(np.sum(weights), 1, delta=5 * epsilon)\n",
        "        self.assertTrue(np.all(weights >= -epsilon), f'\\nSome negative weights: {weights[np.invert(weights >= 0)]}')\n",
        "        self.assertLessEqual(len(chosen_indices), d + 1)\n",
        "        self.assertTrue(np.allclose(data.dot(weights), data[:, chosen_indices].dot(new_weights), atol=1e-10, rtol=0))\n",
        "      \n",
        "print(\"Caratheodory Set Unit-Tests loaded successfully!\")\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Caratheodory Set Unit-Tests loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdxM1J_6RpHF",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "1dabc830-ddb4-4006-e02a-2eaddb31c850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Testing Matrix Coresets Method\n",
        "\n",
        "class TestMatrixCoreset(unittest.TestCase):\n",
        "    \"\"\" A class which contains tests for methods for coreset computation.\"\"\"\n",
        "\n",
        "    def test_outer_products_decomposition(self):\n",
        "        \"\"\"\n",
        "        This test verifies that this `einsum` function computes the outer products of all the matrix's rows.\n",
        "        This means the sum of these outer products must be equal to the original matrix.\n",
        "        \"\"\"\n",
        "        n: int = 7\n",
        "        d: int = 2\n",
        "        A: Matrix = 500 * np.random.randn(n, d)\n",
        "        rows_outer_products: Matrix = np.einsum(\"ij,ik->ijk\", A, A, optimize=True)\n",
        "        self.assertTrue(np.allclose(A.T.dot(A), rows_outer_products.sum(axis=0), atol=1e-8, rtol=0))\n",
        "\n",
        "    def test_python_fast_matrix_coresets(self):\n",
        "        \"\"\"\n",
        "        This test verifies the 'coreset' of the matrix has the same Gram matrix as the original matrix, i.e\n",
        "        :math:`L^{T}L=A^{T}A`.\n",
        "        \"\"\"\n",
        "        n: int = 500\n",
        "        d: int = 7\n",
        "        clusters_count: int = 2 * (d + 1) ** 2 + 2\n",
        "        A: Matrix = 500 * np.random.randn(n, d)\n",
        "        reduced_mat: Matrix = create_coreset_fast_caratheodory(A, clusters_count)\n",
        "        self.assertTrue(np.allclose(reduced_mat.T.dot(reduced_mat), A.T.dot(A), atol=1e-6, rtol=0))\n",
        "        \n",
        "print(\"Coreset Unit-Tests loaded successfully!\")\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coreset Unit-Tests loaded successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYMajC0BfGCo",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "adf983fe-2c9a-48da-e9d7-ddfcd2bc8431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#@title Run all Unit-Tests\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_fast_python_Caratheodory_set (__main__.TestCaratheodorySet) ... ok\n",
            "test_python_Caratheodory_set (__main__.TestCaratheodorySet) ... ok\n",
            "test_outer_products_decomposition (__main__.TestMatrixCoreset) ... ok\n",
            "test_python_fast_matrix_coresets (__main__.TestMatrixCoreset) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.488s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fdc43b319e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    }
  ]
}